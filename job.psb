#!/bin/sh

### Note that the first line tells the system that this is a shell script. 
### Any lines started with #PBS are not comments,
### but the job attributes for PBS Pro to read.

### Specify the amount of time required (maximum 24 hours)
### If you pay 5K SGD monthly then you can require 240 hours for a job :)
#PBS -l walltime=24:00:00

### Select the correct node to queue. 
### Currently, there are two type of nodes in ASPIRE 2A, normal and ai.
### Each normal node has 4 gpus, while each ai node has 8 gpus.
#PBS -q normal
### PBS -q ai

### Request a single GPU
### The "select=1" specifies the number of nodes
### Make the ncpus value 16 times the ngpus value, e.g. select=1:ncpus=16:ngpus=1
#PBS -l select=1:ncpus=16:ngpus=1

### Request multiple GPUs
### Note that each normal node has 4 gpus, while each ai node has 8 gpus.
### If you wanna queue for the normal node with 8 gpus, then use
### #PBS -l select=2:ncpus=64:ngpus=4
### If you wanna queue for the ai node with 8 gpus, then use
### #PBS -l select=1:ncpus=128:ngpus=8

### Specify the project code
### e.g. 41000001 was the pilot project code
### Job will not submit unless this is changed
### The project code, if any, should appear in the welcome information 
### in the console once you logged into your NSCC account.
### A personal code may queue forever.
#PBS -P 12001577

### Specify the name for job
### They records the standard outputs and errors during the job execution.
#PBS -N emo_img

### Standard output by default goes to file $PBS_JOBNAME.o$PBS_JOBID
### Standard error by default goes to file $PBS_JOBNAME.e$PBS_JOBID
### To merge standard output and error use the following
#PBS -j oe


### Start of commands to be run

### Change to directory where the job was submitted
cd "$PBS_O_WORKDIR" || exit $?

### Since our goal is to run the python code, and
### we have built a personal container, we could simply
### run the job by one line. The instructions for each segment
### are provided below.

### Print the environment variables and GPU dashbroad. When the job is finished, you can find them
### from the JOBNAME.oJOBID file.
printenv
nvidia-smi

###### If you use anaconda, not singularity container, see below
###### If you use anaconda, not singularity container, see below
###### If you use anaconda, not singularity container, see below

module load anaconda3/2022.10
conda activate your_env
python ~/tutorial/main.py
### python ~/tutorial/main.py --batch_size $bs --epochs $e

###### If you use anaconda, not singularity container, see above
###### If you use anaconda, not singularity container, see above
###### If you use anaconda, not singularity container, see above


###### If you use singularity container, not anaconda, see below
###### If you use singularity container, not anaconda, see below
###### If you use singularity container, not anaconda, see below

### Bind the scratch directory to singularity so that it can find it.
### Don't change them!
export SINGULARITY_BIND="/scratch:/scratch,/home/project:/data/project"
module load singularity

### Manually assign index to Singularity
export CUDA_VISIBLE_DEVICES=0
### Note that if you applied more gpus, then using export CUDA_VISIBLE_DEVICES=0,1,2,3 for 4 gpus.
### export CUDA_VISIBLE_DEVICES=0,1,2,3

### If you choose to install your Python package on-the-go, uncomment the next line to install your packages.
### singularity run ~/scratch/pt113.sif python -m pip install --user mne pandas seaborn

singularity run --nv  ~/scratch/pt113.sif python ~/tutorial/main.py
### singularity run --nv  ~/scratch/my.sif python ~/tutorial/main.py --batch_size $bs --epochs $e

###### If you use singularity container, not anaconda, see above
###### If you use singularity container, not anaconda, see above
###### If you use singularity container, not anaconda, see above


### The "singularity run --nv my.sif" means
### to use singularity to run "my.sif" container, with
### nvidia cuda support enabled.

### The "python main.py" is exactly the same
### as we did in daily PhD struggle, which is, to run the code.
### When arguments are needed, we indicate them using the dollar symbol $v1 so that later we can feed them to job.psb.

### Some other commands that may be useful are provided below.
### Print the environment variable of NSCC PBS Pro. So 
### that you will know what the goddammit variables in
### https://help.nscc.sg/wp-content/uploads/2016/08/PBS_Professional_Quick_Reference.pdf